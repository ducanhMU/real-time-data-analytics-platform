version: "3.8"

services:
  postgres:
    image: postgres:14
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "wal_sender_timeout=0"
      - "-c"
      - "max_connections=200"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgresql/init.sql:/docker-entrypoint-initdb.d/init.sql # for data-generator
      - ./postgresql/create_publication.sql:/docker-entrypoint-initdb.d/create_publication.sql # for debezium
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d jadc2_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    healthcheck:
      test: nc -z kafka 9092 || exit 1
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    volumes: # note
      - kafka_data:/var/lib/kafka/data
    networks:
      - jadc2

  debezium:
    image: debezium/connect:2.3
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      # kafka connect settings
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "1"
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses

      # converters
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      # connect internal topics replication factors
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1

      # connect worker task configs
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      TASKS_MAX: 1

      # enable debezium scripting
      ENABLE_DEBEZIUM_SCRIPTING: "true"

      # logging
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
    volumes:
      - ./debezium/register-postgres.json:/kafka/config/register-postgres.json:ro
    command: >
      bash -c '
        echo "Waiting for Kafka..."
        cub kafka-ready -b kafka:9092 1 30

        echo "Starting Kafka Connect..."
        /docker-entrypoint.sh start &

        echo "Waiting for Connect to start..."
        until curl -s -o /dev/null -w "%{http_code}" http://localhost:8083/connectors | grep -q 200; do
          sleep 2
        done

        echo "Registering connector..."
        curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/ -d @/kafka/config/register-postgres.json || true

        wait
      '
    healthcheck:
      test: curl -f http://localhost:8083/ || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  kafka-ui: # kafka console
    image: provectuslabs/kafka-ui:latest
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    healthcheck:
      test: curl -f http://localhost:8080 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  data-generator:
    build:
      context: ./data_generator
      dockerfile: Dockerfile
    environment:
      POSTGRES_DB: jadc2_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      CONTINUOUS_RUN: "true"
      LOOP_INTERVAL_SECONDS: 300
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - jadc2

  jobmanager: # flink jobmanager
    build:
      context: ./flink
      dockerfile: Dockerfile
    ports:
      - "6123:6123" # Flink RPC
      - "8081:8081"   # Flink Web UI
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
    depends_on:
      kafka:
        condition: service_healthy
      hms:
        condition: service_started
    volumes:
      - ./flink/conf/hive-site.xml:/opt/flink/conf/hive-site.xml:ro
    networks:
      - jadc2

  taskmanager: # flink task manager
    build:
      context: ./flink
      dockerfile: Dockerfile
    depends_on:
      - jobmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 20
    volumes:
      - ./flink/conf/hive-site.xml:/opt/flink/conf/hive-site.xml:ro
    networks:
      - jadc2

  sql-client: # flink sql client
    build:
      context: ./flink
      dockerfile: Dockerfile
    depends_on:
      - jobmanager
    entrypoint: >
      /bin/bash -c "
      echo 'Waiting for JobManager RPC...'; while ! nc -z jobmanager 6123; do sleep 1; done;
      echo 'All services ready. Launching SQL Client...';
      /opt/flink/bin/sql-client.sh -Dexecution.target=remote -Djobmanager.rpc.address=jobmanager -Djobmanager.rpc.port=6123 -f /opt/flink/kafka_to_kafka.sql
      "
    volumes:
      - ./flink/conf/hive-site.xml:/opt/flink/conf/hive-site.xml:ro
      - ./flink/kafka_to_iceberg.sql:/opt/flink/kafka_to_iceberg.sql:ro
      - ./flink/kafka_to_kafka.sql:/opt/flink/kafka_to_kafka.sql:ro
    networks:
      - jadc2

  hms-db: # hive-metastore backend db
    image: postgres:14
    environment:
      POSTGRES_DB: hms
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_INITDB_ARGS: --data-checksums
      POSTGRES_HOST_AUTH_METHOD: trust
    command: ["postgres", "-c", "wal_level=logical"]
    ports:
      - "5433:5432"
    volumes: # persist data
      - hms_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d hms"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  hms:
    build:
      context: ./hive-metastore
      dockerfile: Dockerfile
    hostname: hms
    depends_on:
      minio:
        condition: service_healthy
      hms-db:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      HMS_LOGLEVEL: INFO
      HIVE_METASTORE_JDBC_URL: "jdbc:postgresql://hms-db:5432/hms"
      HIVE_METASTORE_USER: "hive"
      HIVE_METASTORE_PASSWORD: "hive"
    command: >
      bash -c "
        /opt/hive-metastore/bin/schematool -dbType postgres -info -userName hive -passWord hive -url jdbc:postgresql://hms-db:5432/hms >/dev/null 2>&1 ||
        /opt/hive-metastore/bin/schematool -dbType postgres -initSchema -userName hive -passWord hive -url jdbc:postgresql://hms-db:5432/hms;
        /opt/hive-metastore/bin/hive --service metastore
      "

    volumes:
      - ./hive-metastore/conf/hive-site.xml:/opt/hive-metastore/conf/hive-site.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "nc -z hms 9083 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  minio:
    image: minio/minio
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
    ports:
      - "9001:9001"
      - "9000:9000"
    command: ["server", "/data", "--console-address", ":9001"]
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/minio/health/live || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  mc:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER:-admin} ${MINIO_ROOT_PASSWORD:-password};
      mc mb -p myminio/warehouse || true;
      mc ls myminio/warehouse;
      tail -f /dev/null
      "
    networks:
      - jadc2

  pinot-zookeeper:
    image: zookeeper:3.9.2
    container_name: pinot-zookeeper
    hostname: pinot-zookeeper
    restart: unless-stopped
    ports:
      - "2182:2181" # avoid conflict with another zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: echo srvr | nc pinot-zookeeper 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  pinot-controller:
    image: apachepinot/pinot:1.3.0
    container_name: pinot-controller
    restart: unless-stopped
    entrypoint: [ "sh", "-c" ]
    command: |
      "
        /opt/pinot/bin/pinot-admin.sh StartController -zkAddress pinot-zookeeper:2181 &
        sleep 15;
        chmod +x /opt/pinot/configs/register_pinot.sh;
        sh /opt/pinot/configs/register_pinot.sh;
        wait
      "
    depends_on:
      pinot-zookeeper:
        condition: service_healthy
    ports:
      - "9002:9000" # avoid conflict with minio web ui
    volumes:
      - ./pinot:/opt/pinot/configs
    environment:  
      # Enable Basic Authentication for Controller  
      - controller.admin.access.control.factory.clas 10+s=org.apache.pinot.controller.api.access.BasicAuthAccessControlFactory
      - controller.admin.access.control.principals=admin,superset
      - controller.admin.access.control.principals.admin.password=password
      - controller.admin.access.control.principals.superset.password=password
      - controller.admin.access.control.principals.superset.permissions=read
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  pinot-broker:
    image: apachepinot/pinot:1.3.0
    command: "StartBroker -zkAddress pinot-zookeeper:2181"
    container_name: pinot-broker
    restart: unless-stopped
    depends_on:
      pinot-zookeeper:
        condition: service_healthy
    ports:
      - "8099:8099"
    environment:  
      # Enable Basic Authentication for Broker  
      - pinot.broker.access.control.class=org.apache.pinot.broker.broker.BasicAuthAccessControlFactory
      - pinot.broker.access.control.principals=admin,superset
      - pinot.broker.access.control.principals.admin.password=password
      - pinot.broker.access.control.principals.superset.password=password
      - pinot.broker.access.control.principals.superset.permissions=read
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8099/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  pinot-server:
    image: apachepinot/pinot:1.3.0
    command: >
      StartServer -zkAddress pinot-zookeeper:2181
      -serverHost pinot-server
      -serverPort 8098
      -serverAdminPort 8097
    container_name: pinot-server
    hostname: pinot-server
    restart: unless-stopped
    depends_on:
      pinot-zookeeper:
        condition: service_healthy
    ports:
      - "8098:8098"
      - "8097:8097"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8097/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - pinot_server_data:/var/pinot/data
    networks:
      jadc2:
        aliases:
          - pinot-server

  pinot-minion:
    image: apachepinot/pinot:1.3.0
    command: "StartMinion -zkAddress pinot-zookeeper:2181"
    container_name: pinot-minion
    restart: unless-stopped
    depends_on:
      pinot-zookeeper:
        condition: service_healthy
    volumes:
      - pinot_minion_data:/var/pinot/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9514/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    networks:
      - jadc2
  superset:
    image: apachepinot/pinot-superset:latest
    container_name: jadc2-superset
    depends_on:
      - pinot-broker
    ports:
      - "8088:8088"
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-g0/z2KTBZBygSjh/D5t527EbGu6q1Sl7mC0IKXcZD8oo5uKLo3RqmWbR}
      
    volumes:
      - superset_home:/app/superset_home
    command: >
      bash -c "
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@example.com --password admin || true &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088 --with-threads --reload
      "
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8088 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - jadc2

  # clickhouse:
  #   image: clickhouse/clickhouse-server:latest
  #   container_name: jadc2-clickhouse
  #   ports:
  #     - "8123:8123" # http interface
  #     - "9002:9000" # native client interface
  #   environment:
  #     CLICKHOUSE_USER: ${CLICKHOUSE_USER:-admin}
  #     CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-password}
  #   volumes:
  #     - clickhouse_data:/var/lib/clickhouse
  #     - ./clickhouse/create_clickhouse_table.sql:/docker-entrypoint-initdb.d/create_clickhouse_table.sql:ro # ...?
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:8123 || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   networks:
  #     - jadc2_network

  # dremio:
  #   platform: linux/x86_64
  #   image: dremio/dremio-oss:25.2
  #   container_name: jadc2-dremio
  #   ports:
  #     - "9047:9047"
  #     - "31010:31010"
  #     - "32010:32010"
  #   environment:
  #     DREMIO_JAVA_SERVER_EXTRA_OPTS: -Dpaths.dist=file:///opt/dremio/data/dist
  #     MINIO_ENDPOINT: http://minio:9000
  #     MINIO_ACCESS_KEY: admin
  #     MINIO_SECRET_KEY: password
  #   volumes:
  #     - dremio_data:/opt/dremio/data
  #     - dremio_conf:/opt/dremio/conf
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:9047 || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   networks:
  #     - jadc2_network

  # dbt:
  #     image: python:3.9-slim
  #     container_name: jadc2-dbt
  #     working_dir: /workspace
  #     volumes:
  #       - ./dbt:/workspace:ro
  #     entrypoint: ["/bin/bash", "-c"]
  #     command: >
  #       "pip install dbt-core dbt-postgres || true;
  #       echo 'dbt container ready. Run: docker exec -it jadc2-dbt dbt run' ;
  #       tail -f /dev/null"
  #     depends_on:
  #       - dremio
  #       - minio
  #     networks:
  #       - jadc2_network

  # airflow-db:
  #     image: postgres:14
  #     container_name: jadc2-airflow-db
  #     environment:
  #       POSTGRES_DB: airflow
  #       POSTGRES_USER: ${AIRFLOW_USER:-airflow}
  #       POSTGRES_PASSWORD: ${AIRFLOW_PASSWORD:-airflow}
  #     volumes:
  #       - airflow_postgres_data:/var/lib/postgresql/data
  #     healthcheck:
  #       test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_USER:-airflow} -d airflow"]
  #       interval: 10s
  #       timeout: 5s
  #       retries: 5
  #     networks:
  #       - jadc2_network

  # airflow-webserver:
  #   build:
  #     context: ./airflow
  #     dockerfile: Dockerfile
  #   container_name: jadc2-airflow-webserver
  #   depends_on:
  #     airflow-db:
  #       condition: service_healthy
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: SequentialExecutor
  #     AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_USER:-airflow}:${AIRFLOW_PASSWORD:-airflow}@jadc2-airflow-db:5432/airflow
  #     AIRFLOW__WEBSERVER__PORT: 8082
  #     AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  #     AIRFLOW__CORE__FERNET_KEY: "VKgYkKkKI8G5bxqJd2aTVvGgugpNElZkIhF0ZVwMPs0="
  #     AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: "Asia/Ho_Chi_Minh"
  #   ports:
  #     - "8082:8080" # 8082 ?
  #   volumes:
  #     - ./airflow/dags:/opt/airflow/dags
  #     - ./dbt/dremio_dbt:/opt/airflow/dbt # dremio ?
  #     - ./airflow/plugins:/opt/airflow/plugins
  #     - ./airflow/logs:/opt/airflow/logs
  #   command: >
  #     bash -c "airflow db upgrade && \
  #              airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && \
  #              airflow webserver -p 8082"
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:8082/health || exit 1"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 5
  #   networks:
  #     - jadc2_network

  # airflow-scheduler:
  #   build:
  #     context: ./airflow
  #     dockerfile: Dockerfile
  #   container_name: jadc2-airflow-scheduler
  #   depends_on:
  #     airflow-db:
  #       condition: service_healthy
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: SequentialExecutor
  #     AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_USER:-airflow}:${AIRFLOW_PASSWORD:-airflow}@jadc2-airflow-db:5432/airflow
  #     AIRFLOW__CORE__FERNET_KEY: "VKgYkKkKI8G5bxqJd2aTVvGgugpNElZkIhF0ZVwMPs0="
  #     AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  #     AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: "Asia/Ho_Chi_Minh"
  #   volumes:
  #     - ./airflow/dags:/opt/airflow/dags
  #     - ./dbt/dremio_dbt:/opt/airflow/dbt # dremio ?
  #     - ./airflow/plugins:/opt/airflow/plugins
  #     - ./airflow/logs:/opt/airflow/logs
  #   command: bash -c "airflow db upgrade && airflow scheduler"
  #   networks:
  #     - jadc2_network

  # chatbot:
  #   build:
  #     context: ./chatbot_clickhouse
  #     dockerfile: Dockerfile
  #   container_name: jadc2-chatbot
  #   depends_on:
  #     clickhouse:
  #       condition: service_healthy
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     CLICKHOUSE_HOST: jadc2-clickhouse
  #     CLICKHOUSE_USER: ${CLICKHOUSE_USER:-admin}
  #     CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-password}
  #     CLICKHOUSE_PORT: 8123
  #   volumes:
  #     - ./chatbot_clickhouse/chat_ui.py:/app/chat_ui.py
  #     - ./chatbot_clickhouse/langchain_cli.py:/app/langchain_cli.py
  #     - ./chatbot_clickhouse/requirements.txt:/app/requirements.txt
  #   command: >
  #     bash -c "pip install -r /app/requirements.txt && python /app/chat_ui.py"
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:8000 || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   networks:
  #     - jadc2_network

  

  # chatbot-pinot:
  #   build: ./chatbot_pinot
  #   container_name: chatbot-pinot
  #   environment:
  #     PINOT_BROKER: pinot-broker
  #     PINOT_PORT: 8099
  #   volumes:
  #     - ./chatbot_pinot:/usr/src/app
  #   working_dir: /usr/src/app
  #   command: ["python", "app.py"]
  #   depends_on:
  #     - pinot-broker
  #   ports:
  #     - "8501:8501"
  #   networks:
  #     - jadc2_network

volumes:
  postgres_data:
  kafka_data:
  minio_data:
  hms_data:
  pinot_minion_data:
  pinot_server_data:
  # dremio_data:
  # dremio_conf:
  # airflow_postgres_data:
  # clickhouse_data:
  superset_home:

networks:
  jadc2:
    name: jadc2
    driver: bridge